# -*- coding: utf-8 -*-
"""Cliniscan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BcgF6bY0tJDKM0zu4e7nLyljuvqVJ31X

**1-Data Preparation & Environment Setup**
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pydicom
import os
import cv2
import pydicom
import pandas as pd

def dicom_to_png(dicom_path: str, png_path: str) -> bool:
    try:
        ds = pydicom.dcmread(dicom_path)
        img = ds.pixel_array

        img_norm = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)
        img_uint8 = img_norm.astype("uint8")

        cv2.imwrite(png_path, img_uint8)
        return True
    except Exception as e:
        print(f"Failed to convert {dicom_path}: {e}")
        return False


def convert_dataset(dicom_dir: str, output_dir: str, annotations_csv: str):
    os.makedirs(output_dir, exist_ok=True)

    annotations = pd.read_csv(annotations_csv)

    for _, row in annotations.iterrows():
        dicom_file = os.path.join(dicom_dir, f"{row['image_id']}.dicom")
        png_file = os.path.join(output_dir, f"{row['image_id']}.png")

        if os.path.exists(dicom_file):
            success = dicom_to_png(dicom_file, png_file)
            if success:
                print(f"Converted: {dicom_file} ‚Üí {png_file}")
        else:
            print(f"Missing file: {dicom_file}")

"""**2- Baseline Model Integration & Training**"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import pandas as pd
import numpy as np
import cv2
from PIL import Image

class VinDrCXRDataset(Dataset):
    def __init__(self, df, img_dir, transform=None, task='classification'):
        self.df = df
        self.img_dir = img_dir
        self.transform = transform
        self.task = task

        self.labels = sorted(df['label'].unique())
        self.label_to_id = {label: i for i, label in enumerate(self.labels)}

        self.grouped_images = self.df.groupby('image_id')
        self.image_ids = list(self.grouped_images.groups.keys())

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_path = os.path.join(self.img_dir, f"{img_id}.png")

        image = Image.open(img_path).convert("RGB")

        annotations = self.grouped_images.get_group(img_id)

        if self.task == 'classification':

            label_vector = torch.zeros(len(self.labels), dtype=torch.float)
            for _, row in annotations.iterrows():
                label_vector[self.label_to_id[row['label']]] = 1.0

            if self.transform:
                image = self.transform(image)

            return image, label_vector

        elif self.task == 'detection':

            boxes = []
            labels = []
            for _, row in annotations.iterrows():
                boxes.append([row['x_min'], row['y_min'], row['x_max'], row['y_max']])
                labels.append(self.label_to_id[row['label']])

            target = {}
            target['boxes'] = torch.tensor(boxes, dtype=torch.float32)
            target['labels'] = torch.tensor(labels, dtype=torch.int64)

            if self.transform:
                image = self.transform(image)

            return image, target

"""**3-Model Development & Baseline Training**"""

import torch
import torch.nn as nn
from torchvision.models import resnet18, ResNet18_Weights

class ResNetClassifier(nn.Module):
    def __init__(self, num_classes):
        super(ResNetClassifier, self).__init__()
        # Load a pre-trained ResNet-18 model
        self.model = resnet18(weights=ResNet18_Weights.DEFAULT)
        # Modify the final fully connected layer for your number of classes
        num_ftrs = self.model.fc.in_features
        self.model.fc = nn.Linear(num_ftrs, num_classes)

    def forward(self, x):
        return self.model(x)

if __name__ == "__main__":
    num_classes = 5 # Example number of classes
    model = ResNetClassifier(num_classes)

    # Check model output
    dummy_input = torch.randn(1, 3, 256, 256)
    output = model(dummy_input)
    print(f"Model output shape: {output.shape}")

"""**4. Training (train_classification.py)**"""

import zipfile, os
from google.colab import drive

drive.mount('/content/drive', force_remount=True)

zip_path = "/content/drive/MyDrive/ColabDatasets/archive(3).zip"
extract_path = "/content/dataset"

if not os.path.exists(extract_path) or not os.listdir(extract_path):
    os.makedirs(extract_path, exist_ok=True)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print(f"‚úÖ Dataset extracted to: {extract_path}")
else:
    print(f"‚ÑπÔ∏è Dataset already exists at: {extract_path}")

print("üìÇ Dataset structure (first 40 lines):")
!ls -R /content/dataset | head -40

"""**5-Detection Model**"""

# Install ultralytics
!pip install ultralytics --quiet

import os
import random
import torch
import zipfile
import shutil
from ultralytics import YOLO
from google.colab import drive

# Check torch & GPU
print("‚úÖ Torch version:", torch.__version__)
print("‚úÖ GPU available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("‚úÖ GPU name:", torch.cuda.get_device_name(0))

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Path to your dataset zip
zip_path = "/content/drive/MyDrive/ColabDatasets/archive(3).zip"
extract_path = "/content/dataset"

# Extract dataset if not already done
if not os.path.exists(extract_path) or not os.listdir(extract_path):
    os.makedirs(extract_path, exist_ok=True)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print(f"‚úÖ Dataset extracted to {extract_path}")
else:
    print("‚ÑπÔ∏è Dataset already available.")

# Reorganize dataset for YOLOv8 classification (assuming single class 'cat')
train_dir = os.path.join(extract_path, "train")
val_dir = os.path.join(extract_path, "val")

cat_train_dir = os.path.join(train_dir, "cat")
os.makedirs(cat_train_dir, exist_ok=True)

# Move all images into class folder
for f in os.listdir(train_dir):
    if f.lower().endswith(('.jpg', '.jpeg', '.png')):
        shutil.move(os.path.join(train_dir, f), os.path.join(cat_train_dir, f))

# Create val dir and class subdir
os.makedirs(val_dir, exist_ok=True)
cat_val_dir = os.path.join(val_dir, "cat")
os.makedirs(cat_val_dir, exist_ok=True)

# Split 10% into val
images = os.listdir(cat_train_dir)
random.shuffle(images)
num_val = max(1, int(0.1 * len(images)))

for img in images[:num_val]:
    shutil.move(os.path.join(cat_train_dir, img), os.path.join(cat_val_dir, img))

print("‚úÖ Dataset reorganized for YOLOv8 classification.")

# Show dataset structure
!ls -R /content/dataset | head -40

# Load pretrained YOLOv8 classification model
model = YOLO("yolov8n-cls.pt")

# Start training
model.train(
    data="/content/dataset/chest_xray/chest_xray",  # correct path with train/ and val/
    epochs=20,
    imgsz=224,
    batch=32,
    name="classification_model_fixed"

)

"""**6-Visualization**"""

# PART 1: SETUP
# ===================================================================
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Install grad-cam library
!pip install -q grad-cam

# Import all necessary libraries
import os
import torch
from torchvision import models, transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image

# ===================================================================
# PART 2: VISUALIZATION
# ===================================================================

# Define the dataset path to the NORMAL folder.
dataset_path = "/content/drive/MyDrive/ColabDatasets/train/NORMAL"

try:
    # --- Step 1: Load and Preprocess a Sample Image ---
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"‚ùå ERROR: The path '{dataset_path}' was not found. Please double-check the folder name and location in your Google Drive.")

    image_files = os.listdir(dataset_path)
    if not image_files:
        raise ValueError(f"‚ùå ERROR: The directory '{dataset_path}' is empty. Please ensure it contains image files.")

    sample_image_path = os.path.join(dataset_path, image_files[0])
    print(f"‚úÖ Using sample image: {sample_image_path}")

    # Load and convert the image to RGB format.
    img = Image.open(sample_image_path).convert('RGB')

    # Define the transformations required for the pretrained model.
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    input_tensor = transform(img).unsqueeze(0)

    # --- Step 2: Load a Pretrained Model and Setup Grad-CAM ---
    # Use a pretrained ResNet18 model (trained on ImageNet).
    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
    model.eval() # Set the model to evaluation mode.

    # Define the target layers for Grad-CAM.
    # We use the last convolutional layer, as it captures high-level features.
    target_layers = [model.layer4[-1]]
    cam = GradCAM(model=model, target_layers=target_layers)

    # --- Step 3: Generate the Grad-CAM Heatmap ---
    # Create the original image in the format required for visualization.
    rgb_img = np.array(img.resize((224, 224))) / 255.0

    # Generate the heatmap.
    grayscale_cam = cam(input_tensor=input_tensor)[0]

    # Overlay the heatmap on the original image.
    visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)

    # --- Step 4: Plot the Results ---
    print("\nDisplaying results...")
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

    ax1.imshow(rgb_img)
    ax1.set_title('Original Image')
    ax1.axis('off')

    ax2.imshow(visualization)
    ax2.set_title('Grad-CAM Heatmap')
    ax2.axis('off')

    plt.show()

except FileNotFoundError as e:
    print(e)
except ValueError as e:
    print(e)
except Exception as e:
    print(f"An unexpected error occurred: {e}")

"""**7-Deployment**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import torch
# import torch.nn as nn
# from torchvision import transforms, models
# from PIL import Image
# import subprocess
# import threading
# import time
# import os
# from pyngrok import ngrok, conf
# 
# # --- Pyngrok Configuration and Deployment Logic ---
# # Set the port for the Streamlit app
# STREAMLIT_PORT = 8501
# 
# # **IMPORTANT**: SET YOUR NGROK AUTH TOKEN HERE
# # Get it from your ngrok dashboard at https://dashboard.ngrok.com/get-started/your-authtoken
# NGROK_AUTH_TOKEN = "YOUR_NGROK_AUTH_TOKEN"
# 
# def start_streamlit_process():
#     """
#     Starts the Streamlit app as a background process.
#     The filename is now explicitly 'app.py' to avoid NameError in notebooks.
#     """
#     cmd = ["streamlit", "run", "app.py", "--server.port", str(STREAMLIT_PORT), "--server.headless=True"]
#     proc = subprocess.Popen(cmd)
#     return proc
# 
# def start_ngrok_tunnel(port):
#     """Starts an ngrok tunnel to the specified port."""
#     try:
#         if NGROK_AUTH_TOKEN == "YOUR_NGROK_AUTH_TOKEN":
#             print("\n\n--- NGROK SETUP FAILED ---")
#             print("Please get your ngrok auth token from https://dashboard.ngrok.com/get-started/your-authtoken")
#             print("and replace 'YOUR_NGROK_AUTH_TOKEN' in the code.")
#             return None
# 
#         conf.get_default().auth_token = NGROK_AUTH_TOKEN
# 
#         # Connect to the Streamlit port
#         public_url = ngrok.connect(port, proto="http")
#         return public_url
#     except Exception as e:
#         print(f"Failed to start ngrok tunnel: {e}")
#         return None
# 
# # Check if the code is being run via `streamlit run` or directly
# # This allows the same script to function as the app and the deployment runner
# if "RUNNING_IN_STREAMLIT" not in os.environ:
#     os.environ["RUNNING_IN_STREAMLIT"] = "true"
# 
#     print("\n--- Deployment Script Running ---")
#     print("Please wait while the Streamlit app and ngrok tunnel are set up...")
# 
#     # Start the Streamlit app in a background thread
#     threading.Thread(target=start_streamlit_process).start()
# 
#     # Wait for the app to start and then start the ngrok tunnel
#     time.sleep(5) # Give Streamlit a moment to start
#     public_url = start_ngrok_tunnel(STREAMLIT_PORT)
# 
#     if public_url:
#         print("\n--- APP SUCCESSFULLY DEPLOYED ---")
#         print(f"Public URL: {public_url}\n\n")
#         print("Please click on the URL above to access your web application.")
#         print("Press Ctrl+C to stop the process.")
# 
#     # Keeps the main process alive so the tunnels stay open
#     try:
#         while True:
#             time.sleep(1)
#     except KeyboardInterrupt:
#         print("\nShutting down...")
#         ngrok.kill()
#         print("Shutdown complete.")
#     st.stop()
# 
# # --- Streamlit App (Core Logic) ---
# # This part of the code will only execute when Streamlit runs the script
# # ---------------------------
# # Dummy Model (for a runnable example)
# # ---------------------------
# @st.cache_resource
# def load_model():
#     """
#     Loads a dummy model if 'model.pth' is not found.
#     Replace with your actual trained model.
#     """
#     model = models.resnet18(pretrained=False)
#     model.fc = nn.Linear(model.fc.in_features, 2)
# 
#     try:
#         model.load_state_dict(torch.load("model.pth", map_location="cpu"))
#         st.success("Trained model 'model.pth' loaded successfully.")
#     except FileNotFoundError:
#         st.warning("`model.pth` not found. A dummy model will be used. Please upload your trained model file to the same directory for actual functionality.")
# 
#     model.eval()
#     return model
# 
# model = load_model()
# 
# # ---------------------------
# # Transforms
# # ---------------------------
# transform = transforms.Compose([
#     transforms.Resize((224, 224)),
#     transforms.ToTensor(),
#     transforms.Normalize([0.485, 0.456, 0.406],
#                          [0.229, 0.224, 0.225])
# ])
# 
# # ---------------------------
# # UI
# # ---------------------------
# st.title("ü©ª Chest X-Ray Classification")
# st.write("Upload a Chest X-ray image to classify it.")
# 
# uploaded_file = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])
# 
# if uploaded_file:
#     try:
#         image = Image.open(uploaded_file).convert("RGB")
#         st.image(image, caption="Uploaded Image", use_column_width=True)
# 
#         input_tensor = transform(image).unsqueeze(0)
# 
#         with torch.no_grad():
#             outputs = model(input_tensor)
#             probs = torch.softmax(outputs, dim=1)[0]
#             pred_class = torch.argmax(probs).item()
# 
#         class_names = ["Normal", "Pneumonia"]
#         st.write(f"### Prediction: **{class_names[pred_class]}**")
#         st.write(f"Confidence: {probs[pred_class].item()*100:.2f}%")
#         st.bar_chart({class_names[i]: probs[i].item() for i in range(len(class_names))})
#     except Exception as e:
#         st.error(f"An error occurred during prediction: {e}")

!python app.py